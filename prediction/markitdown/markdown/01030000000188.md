Model

Size

Type

H6 (Avg.) ARC HellaSwag MMLU TruthfulQA Winogrande GSM8K

∼ 11B Alignment-tuned
SOLAR 10.7B-Instruct
∼ 72B
Qwen 72B
Mixtral 8x7B-Instruct-v0.1 ∼ 47B
∼ 34B
Yi 34B-200K
∼ 34B
Yi 34B
∼ 47B
Mixtral 8x7B-v0.1
∼ 70B
Llama 2 70B
∼ 180B
Falcon 180B
∼ 11B
SOLAR 10.7B
∼ 14B
Qwen 14B
∼ 7B
Mistral 7B-Instruct-v0.2
∼ 34B
Yi 34B-Chat
∼ 7B
Mistral 7B

Pretrained
Instruction-tuned
Pretrained
Pretrained
Pretrained
Pretrained
Pretrained
Pretrained
Pretrained
Instruction-tuned
Instruction-tuned
Pretrained

74.20
73.60
72.62
70.81
69.42
68.42
67.87
67.85
66.04
65.86
65.71
65.32
60.97

71.08
65.19
70.22
65.36
64.59
66.04
67.32
69.45
61.95
58.28
63.14
65.44
59.98

88.16
85.94
87.63
85.58
85.69
86.49
87.33
88.86
84.60
83.99
84.88
84.16
83.31

66.21
77.37
71.16
76.06
76.35
71.82
69.83
70.50
65.48
67.70
60.78
74.90
64.16

71.43
60.19
64.58
53.64
56.23
46.78
44.92
45.47
45.04
49.43
68.26
55.37
42.15

83.58
82.48
81.37
82.56
83.03
81.93
83.74
86.90
83.66
76.80
77.19
80.11
78.37

64.75
70.43
60.73
61.64
50.64
57.47
54.06
45.94
55.50
58.98
40.03
31.92
37.83

Table 2: Evaluation results for SOLAR 10.7B and SOLAR 10.7B-Instruct along with other top-performing models.
We report the scores for the six tasks mentioned in Sec. 4.1 along with the H6 score (average of six tasks). We also
report the size of the models in units of billions of parameters. The type indicates the training stage of the model
and is chosen from {Pretrained, Instruction-tuned, Alignment-tuned}. Models based on SOLAR 10.7B are colored
purple. The best scores for H6 and the individual tasks are shown in bold.

We reformatted the instruction datasets with an
Alpaca-styled chat template. For datasets such as
OpenOrca, which are derived from FLAN (Long-
pre et al., 2023), we filter data that overlaps with
the benchmark datasets (see Tab. 8 in Appendix. C
for more information). The alignment datasets are
in the {prompt, chosen, rejected} triplet format.
We preprocess the alignment datasets following
Zephyr (Tunstall et al., 2023).

Evaluation.
In the HuggingFace Open LLM
Leaderboard (Beeching et al., 2023), six types of
evaluation methods are presented: ARC (Clark
et al., 2018), HellaSWAG (Zellers et al., 2019),
MMLU (Hendrycks et al., 2020), TruthfulQA (Lin
et al., 2022), Winogrande (Sakaguchi et al., 2021),
and GSM8K (Cobbe et al., 2021). We utilize these
datasets as benchmarks for evaluation and also re-
port the average scores for the six tasks, e.g., H6.

Model merging. Model merging methods such
as Yadav et al. (2023) can boost model perfor-
mance without further training. We merge some
of the models that we trained in both the instruc-
tion and alignment tuning stages. We implement
our own merging methods although popular open
source also exist such as MergeKit3.

4.2 Main Results

We present evaluation results for our SOLAR
10.7B and SOLAR 10.7B-Instruct models along
with other top-performing models in Tab. 2. SO-
LAR 10.7B outperforms other pretrained models
of similar sizes, such as Qwen 14B and Mistral
7B, which shows that DUS is an effective method
to up-scale base LLMs. Furthermore, despite the

3https://github.com/cg123/mergekit

smaller size, SOLAR 10.7B-Instruct scores the
highest in terms of H6, even surpassing the recent
top-performing open-source LLM Mixtral 8x7B-
Instruct-v0.1 or Qwen 72B. The above results indi-
cate DUS can up-scale models that are capable of
achieving state-of-the-art performance when fine-
tuned. We also report data contamination results
for SOLAR 10.7B-Instruct in Appendix C.

4.3 Ablation Studies

We present ablation studies for both the instruction
and alignment tuning stages.

4.3.1

Instruction Tuning

Ablation on the training datasets. We present
ablation studies using different training datasets
for the instruction tuning in Tab. 3. The ablated
models are prefixed with SFT for supervised fine-
tuning.
‘SFT v1’ only uses the Alpaca-GPT4
dataset, whereas ‘SFT v2’ also uses the OpenOrca
dataset. ‘SFT v3’ uses the Synth. Math-Instruct
dataset along with the datasets used in ‘SFT v2’.
Similarly, ‘SFT v4’ uses the Synth. Math-Instruct
dataset along with the datasets used in ‘SFT v1’.

First, we analyze how Alpaca-GPT4 and
OpenOrca affect the trained models. The first ab-
lated model, ‘SFT v1’, which used only the Alpaca-
GPT4 dataset for training, resulted in 69.15 for H6.
When we add the OpenOrca dataset to train the
second ablated model, ‘SFT v2’, the resulting H6
score is 69.21, which is little change from 69.15 of
‘SFT v1’. However, the task scores vary more as
‘SFT v2’ gets a substantially higher GSM8K score
of 57.32 compared to 52.24 of ‘SFT v1’ but also
gets noticeably lower scores across the board for
ARC, HellaSwag, and TruthfulQA. This seems to

