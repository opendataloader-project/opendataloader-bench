Table 6: Performance comparison amongst the merge candidates. ‘Cand. 1’ and ‘Cand. $_ { 2 } \cdot$ are trained using the same setting as ‘DPO $\mathbf { v } 2 ^ { \bullet }$ and ‘DPO $\mathbf { v } 3 ^ { \mathbf { \cdot } }$ , respectively, but with slightly different hyper-parameters. The best scores for H6 and the individual tasks are shown in bold.   

<table><tr><td>Model</td><td>H6(Avg.)</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Winogrande</td><td>GSM8K</td></tr><tr><td>Cand. 1</td><td>73.73</td><td>70.48</td><td>87.47</td><td>65.73</td><td>70.62</td><td>81.53</td><td>66.57</td></tr><tr><td>Cand. 2</td><td>73.28</td><td>71.59</td><td>88.39</td><td>66.14</td><td>72.50</td><td>81.99</td><td>59.14</td></tr></table>

<table><tr><td>Model</td><td>Merge Method</td><td>H6 (Avg.)</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Winogrande</td><td>GSM8K</td></tr><tr><td>Merge v1</td><td>Average (0.5,0.5)</td><td>74.00</td><td>71.16</td><td>88.01</td><td>66.14</td><td>71.71</td><td>82.08</td><td>64.90</td></tr><tr><td>Merge v2</td><td>Average (0.4,0.6)</td><td>73.93</td><td>71.08</td><td>88.08</td><td>66.27</td><td>71.89</td><td>81.77</td><td>64.52</td></tr><tr><td>Merge v3</td><td>Average (0.6,0.4)</td><td>74.05</td><td>71.08</td><td>87.88</td><td>66.13</td><td>71.61</td><td>82.08</td><td>65.50</td></tr><tr><td>Merge v4</td><td>SLERP</td><td>73.96</td><td>71.16</td><td>88.03</td><td>66.25</td><td>71.79</td><td>81.93</td><td>64.59</td></tr></table>

Table 7: Ablation studies on the different merge methods used for obtaining the final model. We use ‘Cand. 1’ and ‘Cand. 2’ from Tab. 6 as our two models for merging. We name the merged models with the ‘Merge’ prefix to indicate they are merged. The best scores for H6 and the individual tasks are shown in bold.

tively impacted by adding Synth. Math-Alignment. Thus, we can conclude that adding Synth. Math-Alignment is beneficial for H6.

Then, we experiment whether merging ‘DPO v1’ and ‘DPO v2’ is beneficial. Unfortunately, ‘DPO $\mathbf { v } 1 + \mathbf { v } 2 ^ { \flat }$ scores 73.21 in H6, which is worse than ‘DPO v2’. More importantly, the gain in the GSM8K score from adding Synth. Math-Alignment is gone, which is undesirable. One reason for this could be that ‘DPO v2’ is a strict improvement over ‘DPO v1’, unlike the case for merging ‘SFT v3’ and ‘SFT v4’ where the models had different strengths and weaknesses.

Ablation on the SFT base models. When applying DPO, we start from a model that is already instruction tuned ,i.e., the SFT base model and ablate on using different SFT base models. We use Ultrafeedback Clean and Synth. Math-Alignment datasets for this ablation. Each of the ablated models is trained as follows. ‘DPO $\mathbf { v } 2 ^ { \bullet }$ uses ‘SFT v3’ as the base SFT model, while ‘DPO v3’ uses ‘SFT $\mathbf { v } 3 + \mathbf { v } 4 ^ { \prime }$ as the SFT base model instead.

Note that ‘SFT $\mathbf { v } 3 + \mathbf { v } 4 ^ { \mathbf { \prime } }$ has higher scores on all tasks compared to ‘SFT $\mathbf { v } 3 ^ { \mathbf { \cdot } }$ , and the gap is especially large for ARC $\left( + 1 . 4 5 \right)$ and GSM8K $\left( + 2 . 4 3 \right)$ Surprisingly, the two models perform similarly in terms of H6. A closer look at the scores for the individual tasks shows only a small margin in the GSM8K scores, and other task scores show little difference. Thus, the performance gaps in certain tasks in the SFT base models do not always carry over to the alignment-tuned models.

Ablation on different merge methods. From Tab. 3, we saw that merging two models that have different strengths can be beneficial to performance.

To utilize this for the alignment-tuned model as well, we train two models named ‘Cand. 1’ and ‘Cand. 2’ using the same training dataset and SFT base model as ‘DPO v2’ and ‘DPO v3’ but with different hyper-parameters to maximize each model’s respective strengths. We compare ‘Cand. 1’ and ‘Cand. $2 ^ { \cdot }$ in Tab. 6 where we can see that ‘Cand. 1’ has high GSM8K scores but relatively low scores for the other tasks, whereas ‘Cand. $2 ^ { \bullet }$ has low scores for GSM8K but high scores for the other tasks. We merge these two models using various methods and ablate the results in Tab.. 7.

We use two merge methods: 1) Average $( a , b )$ , where a and b denote the weighting for ‘Cand. 1’ and ‘Cand. $_ { 2 } \cdot$ when averaging weights and 2) SLERP (Shoemake, 1985). We use (0.5, 0.5), (0.4, 0.6), and (0.6, 0.4) for Average $( a , b )$ . From Tab. 7, we can see that the different merge methods have little effect on the H6 scores. The scores for the individual tasks also do not differ by much, suggesting that as long as the merge candidates have sufficiently different strengths, the exact merge method may not be as crucial. Thus, we chose ‘Merge v1’ as our SOLAR 10.7B-Instruct model.

# 5 Conclusion

We introduce SOLAR 10.7B and its fine-tuned variant SOLAR 10.7B-Instruct, which are depth upscaled (DUS) models with 10.7 billion parameters. They show superior performance over models like Llama 2, Mistral 7B, and Mixtral-7B-Instruct in essential NLP tasks while maintaining computational efficiency. Thus, DUS is effective in scaling-up highly performant LLMs from smaller ones. With more exploration, DUS could be further improved, paving a new path to efficiently scaling LLMs.